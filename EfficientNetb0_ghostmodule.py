# -*- coding: utf-8 -*-
"""Efficient b0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/193qvl2AOhs0UBMyRoX5S_JXhapQvSx2f
"""

# Commented out IPython magic to ensure Python compatibility.
import keras
import tensorflow as tf
# %tensorflow_version 1.13
import math
from keras.layers import UpSampling2D
import keras.backend as K
from keras.activations import relu



from keras.models import Model
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import Conv2D
from keras.layers import DepthwiseConv2D 
from keras.layers.convolutional import MaxPooling2D
from keras.layers.core import Activation
from keras.layers.core import Flatten
from keras.layers.core import Dropout
from keras.layers.core import Dense
from keras import backend as K
from keras.layers import merge
from keras.layers import Input
from keras.backend import sigmoid
from keras.layers import Add
from keras.layers import Multiply
from keras.layers import ZeroPadding2D
from keras.layers import BatchNormalization as bn
from keras.layers import GlobalAveragePooling2D
from keras.layers import Concatenate
from keras.layers import Lambda
from keras.layers import Dense
def swish(x, beta = 1):
    return (x * sigmoid(beta * x))
from keras.models import Model
def ew(p):
    return int(1.1*p)


from keras.utils.generic_utils import get_custom_objects
from keras.layers import Activation
get_custom_objects().update({'swish': Activation(swish)})
def sliced(x,i,j):
        return x[:,:,:,:i-j]

def ghostmodule(x,exp,ratio=2):
    init_channels=math.ceil(exp / ratio)
    x=Conv2D(init_channels,(1,1))(x)
    dw1=DepthwiseConv2D((1,1))(x)
    dw2=Lambda(lambda t:sliced(t,exp,init_channels))(dw1)
    x=Concatenate(axis=3)([x,dw2])
    return x

from keras.backend import sigmoid
def swish(x, beta = 1):
    return (x * sigmoid(beta * x))

def mbconv(x, expand, squeeze):
  if K.int_shape(x)[3]==ew(expand):
    y=x
  else:
   y=ghostmodule(x,ew(squeeze))
   y=bn(momentum=0.99)(y)
    
  m = ghostmodule(x,ew(expand))
  m = DepthwiseConv2D((3,3), activation='relu',padding='same')(m)
  m=bn(momentum=0.99)(m)
  m=se_block(m,K.int_shape(m)[3])
  m=bn(momentum=0.99)(m)
  m = ghostmodule(m,ew(squeeze))
  m=bn(momentum=0.99)(m)
  final=Add()([m, y])
  final=Activation('relu')(final)
  return bn(momentum=0.99)(final)

def mbconv2a(x, expand, squeeze):
  if K.int_shape(x)[3]==ew(expand):
    y=x
  else:
    y=ghostmodule(x,ew(squeeze))
    y=bn(momentum=0.99)(y)
  m = ghostmodule(x,ew(expand))
  m = DepthwiseConv2D((5,5), activation='relu',padding='same')(m)
  m=bn(momentum=0.99)(m)
  m=se_block(m,K.int_shape(m)[3])
  m=bn(momentum=0.99)(m)
  m = ghostmodule(m,ew(squeeze))
  m=bn(momentum=0.99)(m)
  final=Add()([m, y])
  final=Activation('relu')(final)
  return bn(momentum=0.99)(final)


def mbconv61(t,expand,squeeze,k,stride):
    b=ghostmodule(t,ew(expand))
    c=Conv2D(ew(squeeze),(1,1),strides=(stride,stride))(t)
    c=bn(momentum=0.99)(c)
    b=ZeroPadding2D(1)(b)
    b=DepthwiseConv2D((k,k),activation='relu',strides=(stride,stride))(b)
    b=bn(momentum=0.99)(b)
    b=se_block(b,K.int_shape(b)[3])
    b=bn(momentum=0.99)(b)
    b=ghostmodule(b,ew(squeeze))
    b=bn(momentum=0.99)(b)
    d=Add()([b,c])
    d=Activation('relu')(d)
    return bn(momentum=0.99)(d)   
def mbconv62(t,expand,squeeze,k,stride):
    b=ghostmodule(t,ew(expand))
    c=Conv2D(ew(squeeze),(1,1),strides=(stride,stride))(t)
    c=bn(momentum=0.99)(c)
    b=ZeroPadding2D(2)(b)
    b=DepthwiseConv2D((k,k),activation='relu',strides=(stride,stride))(b)
    b=bn(momentum=0.99)(b)
    b=se_block(b,K.int_shape(b)[3])
    b=bn(momentum=0.99)(b)
    b=ghostmodule(b,ew(squeeze))
    b=bn(momentum=0.99)(b)
    d=Add()([b,c])
    d=Activation('relu')(d)
    return bn(momentum=0.99)(d)
     
def se_block(in_block, ch, ratio=4):
    x = GlobalAveragePooling2D()(in_block)
    x = Dense(ch//ratio, activation='relu')(x)
    x = Dense(ch, activation='relu')(x)
    return Multiply()([in_block,x])

input=Input(shape=(299,299,3))
upsample=UpSampling2D(size=(1,1))(input)



padding=ZeroPadding2D(1)(upsample)
conv1=Conv2D(32,(3,3),activation='relu',strides=2,use_bias=False)(padding)
conv1=bn(momentum=0.99)(conv1)
mbconv1=mbconv(conv1,128,16)
mbconv2=mbconv(mbconv1,80,20)
mbconv3=mbconv61(mbconv2,96,24,3,2)
mbconv4=mbconv2a(mbconv3,128,32)
mbconv5=mbconv62(mbconv4,160,40,5,2)
mbconv6=mbconv(mbconv5,160,40)
mbconv7=mbconv(mbconv6,240,60)
mbconv8=mbconv61(mbconv7,320,80,3,2)
mbconv9=mbconv2a(mbconv8,320,80)
mbconv10=mbconv2a(mbconv9,384,96)
mbconv11=mbconv2a(mbconv10,448,112)
mbconv12=mbconv2a(mbconv11,528,132)
mbconv13=mbconv2a(mbconv12,608,152)
mbconv14=mbconv2a(mbconv13,688,172)
mbconv15=mbconv2a(mbconv14,688,172)
mbconv16=mbconv62(mbconv15,768,192,5,2)
mbconv17=mbconv2a(mbconv16,768,192)
mbconv18=mbconv(mbconv17,1280,320)
mbconv19=mbconv(mbconv18,1280,320)
mbconv20=Conv2D(1280,1)(mbconv19)
mbconv21=keras.layers.MaxPooling2D(pool_size=(5,5),padding='same')(mbconv20)
flatten=Flatten()(mbconv21)
output1=Dense(256, activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(flatten)
dropout=Dropout(0.2)(output1)
output3=Dense(10, activation='softmax', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(dropout)

model = Model(inputs=input, outputs=output3)
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.summary()

from keras.datasets import cifar10
from keras.preprocessing.image import ImageDataGenerator
from keras.utils import np_utils
num_classes=10
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
y_train = np_utils.to_categorical(y_train, num_classes)
y_test = np_utils.to_categorical(y_test, num_classes)

datagen = ImageDataGenerator(
    featurewise_center=True,
    featurewise_std_normalization=True,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,





    
    horizontal_flip=True)

# compute quantities required for featurewise normalization
# (std, mean, and principal components if ZCA whitening is applied)
datagen.fit(x_train)

# fits the model on batches with real-time data augmentation:
model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),
                    steps_per_epoch=len(x_train) / 32, epochs=100)

print(model.evaluate(x=x_test, y=y_test, batch_size=32, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False))





